{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbMg63ssZgWr"
      },
      "outputs": [],
      "source": [
        "1. Ingestion & Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read all 3 files (CSV + JSON) using PySpark."
      ],
      "metadata": {
        "id": "kDDfmYT8bT_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SfgOyt6biAM",
        "outputId": "bdec91ef-efef-4cd8-c999-fce0c575c433"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HRAnalyticsIngestion\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "employees_path = \"/content/drive/MyDrive/employees.csv\"\n",
        "attendance_path = \"/content/drive/MyDrive/attendance.csv\"\n",
        "bonuses_path = \"/content/drive/MyDrive/bonuses.json\"\n",
        "\n",
        "employees_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(employees_path)\n",
        "attendance_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(attendance_path)\n",
        "\n",
        "bonuses_df = spark.read.option(\"multiline\", True).json(bonuses_path)"
      ],
      "metadata": {
        "id": "8YRAyGMZbWBt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show schemas and sample records."
      ],
      "metadata": {
        "id": "zMpazTbJcqV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.printSchema()\n",
        "attendance_df.printSchema()\n",
        "bonuses_df.printSchema()\n",
        "\n",
        "employees_df.show()\n",
        "attendance_df.show()\n",
        "bonuses_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL4Bqwy9ctGH",
        "outputId": "f7b807ee-0ac0-48b6-d0bd-fbfbe8f1aabb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- JoinDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- ManagerID: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- Bonus: long (nullable = true)\n",
            " |-- EmpID: long (nullable = true)\n",
            " |-- Year: long (nullable = true)\n",
            "\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|        1|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|        1|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|        1|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|        1|\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "\n",
            "+-----+----------+-------+\n",
            "|EmpID|      Date| Status|\n",
            "+-----+----------+-------+\n",
            "|    1|2024-04-01|Present|\n",
            "|    1|2024-04-02|Present|\n",
            "|    2|2024-04-01| Absent|\n",
            "|    2|2024-04-02|Present|\n",
            "|    3|2024-04-01|Present|\n",
            "|    3|2024-04-02|Present|\n",
            "|    4|2024-04-01| Absent|\n",
            "|    4|2024-04-02| Absent|\n",
            "|    5|2024-04-01|Present|\n",
            "|    5|2024-04-02|Present|\n",
            "+-----+----------+-------+\n",
            "\n",
            "+-----+-----+----+\n",
            "|Bonus|EmpID|Year|\n",
            "+-----+-----+----+\n",
            "| 5000|    1|2023|\n",
            "| 7000|    2|2023|\n",
            "| 6500|    3|2023|\n",
            "| 6000|    4|2023|\n",
            "| 4000|    5|2023|\n",
            "+-----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count distinct departments."
      ],
      "metadata": {
        "id": "3ph_RSAacxqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_departments = employees_df.select(\"Department\").distinct().count()\n",
        "print(f\"Total distinct departments: {distinct_departments}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5YqJJL9czgl",
        "outputId": "4f3e591b-7920-4060-a676-5b8af7b0f80e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total distinct departments: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. DataFrame Operations"
      ],
      "metadata": {
        "id": "WaIgg91kc5Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a column TenureYears using datediff() and round()"
      ],
      "metadata": {
        "id": "iOssRZe5c-ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import datediff, current_date, round, col\n",
        "\n",
        "employees_df = employees_df.withColumn(\n",
        "    \"TenureYears\",\n",
        "    round(datediff(current_date(), col(\"JoinDate\")) / 365, 2)\n",
        ")\n",
        "\n",
        "employees_df.select(\"EmpID\", \"Name\", \"JoinDate\", \"TenureYears\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPTelDfnc5uC",
        "outputId": "f4122d0e-77eb-4af5-cba4-4bcc82d83d16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+-----------+\n",
            "|EmpID|  Name|  JoinDate|TenureYears|\n",
            "+-----+------+----------+-----------+\n",
            "|    1| Anita|2021-05-01|       4.11|\n",
            "|    2|   Raj|2020-03-15|       5.24|\n",
            "|    3|Simran|2022-07-10|       2.92|\n",
            "|    4| Aamir|2019-11-20|       5.56|\n",
            "|    5| Nisha|2023-01-05|       2.43|\n",
            "+-----+------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate TotalCompensation = Salary + Bonus"
      ],
      "metadata": {
        "id": "q_b_A7HUdFZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_bonus_df = employees_df.join(bonuses_df, on=\"EmpID\", how=\"left\") \\\n",
        "    .withColumn(\"TotalCompensation\", col(\"Salary\") + col(\"Bonus\"))\n",
        "\n",
        "emp_bonus_df.select(\"EmpID\", \"Name\", \"Salary\", \"Bonus\", \"TotalCompensation\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6tR-HNIdI-o",
        "outputId": "09273874-f89e-44f2-a596-775c3fd6bbf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+-----+-----------------+\n",
            "|EmpID|  Name|Salary|Bonus|TotalCompensation|\n",
            "+-----+------+------+-----+-----------------+\n",
            "|    1| Anita| 55000| 5000|            60000|\n",
            "|    2|   Raj| 80000| 7000|            87000|\n",
            "|    3|Simran| 75000| 6500|            81500|\n",
            "|    4| Aamir| 60000| 6000|            66000|\n",
            "|    5| Nisha| 50000| 4000|            54000|\n",
            "+-----+------+------+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter employees with more than 2 years in the company."
      ],
      "metadata": {
        "id": "0EZ9paNpdL3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_bonus_df.filter(col(\"TenureYears\") > 2).select(\"EmpID\", \"Name\", \"TenureYears\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UUGEvc-dOBJ",
        "outputId": "d3c39bf2-765e-422f-dd71-3300ea630714"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+\n",
            "|EmpID|  Name|TenureYears|\n",
            "+-----+------+-----------+\n",
            "|    1| Anita|       4.11|\n",
            "|    2|   Raj|       5.24|\n",
            "|    3|Simran|       2.92|\n",
            "|    4| Aamir|       5.56|\n",
            "|    5| Nisha|       2.43|\n",
            "+-----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show employees who report to a manager"
      ],
      "metadata": {
        "id": "8wcCWsvXdQ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.filter(col(\"ManagerID\").isNotNull()).select(\"EmpID\", \"Name\", \"ManagerID\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IwpkiHOdd0P",
        "outputId": "ed56934e-c281-48fb-9244-111b8ed36e4b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+\n",
            "|EmpID|  Name|ManagerID|\n",
            "+-----+------+---------+\n",
            "|    2|   Raj|        1|\n",
            "|    3|Simran|        1|\n",
            "|    4| Aamir|        1|\n",
            "|    5| Nisha|        1|\n",
            "+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Aggregation"
      ],
      "metadata": {
        "id": "Eb31hKljdgxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average salary per department"
      ],
      "metadata": {
        "id": "Lu1_6dU3dkAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "employees_df.groupBy(\"Department\") \\\n",
        "    .agg(avg(\"Salary\").alias(\"AverageSalary\")) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzcfBdUdi24",
        "outputId": "d2eadc04-b061-456a-b9bb-f640b100a968"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "| Department|AverageSalary|\n",
            "+-----------+-------------+\n",
            "|Engineering|      77500.0|\n",
            "|         HR|      52500.0|\n",
            "|  Marketing|      60000.0|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of employees under each manager."
      ],
      "metadata": {
        "id": "BYDoQ1xeduyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "employees_df.groupBy(\"ManagerID\") \\\n",
        "    .agg(count(\"EmpID\").alias(\"NumEmployees\")) \\\n",
        "    .filter(col(\"ManagerID\").isNotNull()) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlM2An3CdxfS",
        "outputId": "02461183-e63e-4562-90fa-e8b12fafc756"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|ManagerID|NumEmployees|\n",
            "+---------+------------+\n",
            "|        1|           4|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count of absences per employee"
      ],
      "metadata": {
        "id": "XPsKavDtd1Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attendance_df.filter(col(\"Status\") == \"Absent\") \\\n",
        "    .groupBy(\"EmpID\") \\\n",
        "    .agg(count(\"Date\").alias(\"AbsenceCount\")) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1_a4CDSd3BF",
        "outputId": "021d319f-b34a-496c-9779-cee9cfbdfc30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|EmpID|AbsenceCount|\n",
            "+-----+------------+\n",
            "|    4|           2|\n",
            "|    2|           1|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Joins"
      ],
      "metadata": {
        "id": "xblnQxV9d51W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join employees and attendance"
      ],
      "metadata": {
        "id": "cIH-lyAsd9QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, count, col\n",
        "\n",
        "total_days_df = attendance_df.groupBy(\"EmpID\").agg(count(\"Date\").alias(\"TotalDays\"))\n",
        "\n",
        "present_days_df = attendance_df.filter(col(\"Status\") == \"Present\") \\\n",
        "    .groupBy(\"EmpID\") \\\n",
        "    .agg(count(\"Date\").alias(\"PresentDays\"))\n",
        "\n",
        "attendance_rate_df = total_days_df.join(present_days_df, \"EmpID\", \"left\") \\\n",
        "    .withColumn(\"AttendancePercent\", (col(\"PresentDays\") / col(\"TotalDays\")) * 100)\n",
        "\n",
        "attendance_rate_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuWhc_jrd_PT",
        "outputId": "c42aac11-7cd4-4442-ae5d-2a0f86032cc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----------+-----------------+\n",
            "|EmpID|TotalDays|PresentDays|AttendancePercent|\n",
            "+-----+---------+-----------+-----------------+\n",
            "|    1|        2|          2|            100.0|\n",
            "|    3|        2|          2|            100.0|\n",
            "|    5|        2|          2|            100.0|\n",
            "|    4|        2|       NULL|             NULL|\n",
            "|    2|        2|          1|             50.0|\n",
            "+-----+---------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join employees and bonuses"
      ],
      "metadata": {
        "id": "n-yF6nsoeUlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "top_3_employees_df = employees_df.join(bonuses_df, \"EmpID\") \\\n",
        "    .withColumn(\"TotalCompensation\", col(\"Salary\") + col(\"Bonus\")) \\\n",
        "    .orderBy(desc(\"TotalCompensation\")) \\\n",
        "    .limit(3)\n",
        "\n",
        "top_3_employees_df.select(\"EmpID\", \"Name\", \"Department\", \"TotalCompensation\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHQSJN95eU97",
        "outputId": "3765681d-eac9-45ab-a2f2-7ab5f1ee992c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+-----------------+\n",
            "|EmpID|  Name| Department|TotalCompensation|\n",
            "+-----+------+-----------+-----------------+\n",
            "|    2|   Raj|Engineering|            87000|\n",
            "|    3|Simran|Engineering|            81500|\n",
            "|    4| Aamir|  Marketing|            66000|\n",
            "+-----+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Multi-level join"
      ],
      "metadata": {
        "id": "S5DOP-SYebAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_join_df = employees_df.join(bonuses_df, \"EmpID\", \"left\") \\\n",
        "    .join(attendance_df, \"EmpID\", \"left\")\n",
        "\n",
        "multi_join_df.select(\"EmpID\", \"Name\", \"Department\", \"Date\", \"Status\", \"Bonus\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIwJj1y5eblr",
        "outputId": "c8d3d96a-319f-4d8a-f835-0c6356a50a7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+----------+-------+-----+\n",
            "|EmpID|  Name| Department|      Date| Status|Bonus|\n",
            "+-----+------+-----------+----------+-------+-----+\n",
            "|    1| Anita|         HR|2024-04-02|Present| 5000|\n",
            "|    1| Anita|         HR|2024-04-01|Present| 5000|\n",
            "|    2|   Raj|Engineering|2024-04-02|Present| 7000|\n",
            "|    2|   Raj|Engineering|2024-04-01| Absent| 7000|\n",
            "|    3|Simran|Engineering|2024-04-02|Present| 6500|\n",
            "|    3|Simran|Engineering|2024-04-01|Present| 6500|\n",
            "|    4| Aamir|  Marketing|2024-04-02| Absent| 6000|\n",
            "|    4| Aamir|  Marketing|2024-04-01| Absent| 6000|\n",
            "|    5| Nisha|         HR|2024-04-02|Present| 4000|\n",
            "|    5| Nisha|         HR|2024-04-01|Present| 4000|\n",
            "+-----+------+-----------+----------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. String & Date Functions"
      ],
      "metadata": {
        "id": "jUj4Fg15e6V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract year and month from JoinDate."
      ],
      "metadata": {
        "id": "RvwN9QZCe64D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "employees_df = employees_df \\\n",
        "    .withColumn(\"JoinYear\", year(col(\"JoinDate\"))) \\\n",
        "    .withColumn(\"JoinMonth\", month(col(\"JoinDate\")))\n",
        "\n",
        "employees_df.select(\"EmpID\", \"Name\", \"JoinDate\", \"JoinYear\", \"JoinMonth\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZqRp7hqfCMv",
        "outputId": "1bcc4a18-a3d5-4445-acb7-8b6f965b1e11"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+--------+---------+\n",
            "|EmpID|  Name|  JoinDate|JoinYear|JoinMonth|\n",
            "+-----+------+----------+--------+---------+\n",
            "|    1| Anita|2021-05-01|    2021|        5|\n",
            "|    2|   Raj|2020-03-15|    2020|        3|\n",
            "|    3|Simran|2022-07-10|    2022|        7|\n",
            "|    4| Aamir|2019-11-20|    2019|       11|\n",
            "|    5| Nisha|2023-01-05|    2023|        1|\n",
            "+-----+------+----------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mask employee names using regex."
      ],
      "metadata": {
        "id": "FWU2MKhnfD8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "masked_names_df = employees_df.withColumn(\"MaskedName\", regexp_replace(\"Name\", r\"(.)(.*)\", r\"$1****\"))\n",
        "masked_names_df.select(\"EmpID\", \"Name\", \"MaskedName\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUveNgISfGI8",
        "outputId": "b43510b3-0adc-4d3f-ca1f-677e1ef0a849"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+\n",
            "|EmpID|  Name|MaskedName|\n",
            "+-----+------+----------+\n",
            "|    1| Anita|     A****|\n",
            "|    2|   Raj|     R****|\n",
            "|    3|Simran|     S****|\n",
            "|    4| Aamir|     A****|\n",
            "|    5| Nisha|     N****|\n",
            "+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use substring() to create EmpCode like \"EMP001\"."
      ],
      "metadata": {
        "id": "lZAGEw-_fJXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lpad, concat, lit\n",
        "\n",
        "employees_df = employees_df.withColumn(\"EmpCode\", concat(lit(\"EMP\"), lpad(col(\"EmpID\").cast(\"string\"), 3, \"0\")))\n",
        "employees_df.select(\"EmpID\", \"EmpCode\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SvJEK4CfLew",
        "outputId": "f87388cb-7e89-4199-ce1e-3b6dd721530c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|EmpID|EmpCode|\n",
            "+-----+-------+\n",
            "|    1| EMP001|\n",
            "|    2| EMP002|\n",
            "|    3| EMP003|\n",
            "|    4| EMP004|\n",
            "|    5| EMP005|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Conditional & Null Handling"
      ],
      "metadata": {
        "id": "ca868ucjfQZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use when/otherwise to label performance"
      ],
      "metadata": {
        "id": "zbsJ02TZfS6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "performance_df = bonuses_df.withColumn(\n",
        "    \"Performance\",\n",
        "    when(col(\"Bonus\") > 6000, \"High\")\n",
        "    .when((col(\"Bonus\") >= 4000) & (col(\"Bonus\") <= 6000), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "performance_df.select(\"EmpID\", \"Bonus\", \"Performance\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oGOW1HYfV_1",
        "outputId": "19ac7285-246f-44b7-f12c-bd74677ff3a5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+\n",
            "|EmpID|Bonus|Performance|\n",
            "+-----+-----+-----------+\n",
            "|    1| 5000|     Medium|\n",
            "|    2| 7000|       High|\n",
            "|    3| 6500|       High|\n",
            "|    4| 6000|     Medium|\n",
            "|    5| 4000|     Medium|\n",
            "+-----+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle missing ManagerID using fillna"
      ],
      "metadata": {
        "id": "4t2gXzPlfrjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_filled_df = employees_df.fillna({\"ManagerID\": \"No Manager\"})\n",
        "employees_filled_df.select(\"EmpID\", \"Name\", \"ManagerID\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ_WFtrafuI_",
        "outputId": "369ebf77-26d3-4c04-a398-24591165841a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+\n",
            "|EmpID|  Name|ManagerID|\n",
            "+-----+------+---------+\n",
            "|    1| Anita|     NULL|\n",
            "|    2|   Raj|        1|\n",
            "|    3|Simran|        1|\n",
            "|    4| Aamir|        1|\n",
            "|    5| Nisha|        1|\n",
            "+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Spark SQL"
      ],
      "metadata": {
        "id": "upQ3TRvUfwvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and use database hr."
      ],
      "metadata": {
        "id": "BRUtvAoffy8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS hr\")\n",
        "\n",
        "spark.catalog.setCurrentDatabase(\"hr\")"
      ],
      "metadata": {
        "id": "75DLqLsIf1kH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all DataFrames as tables"
      ],
      "metadata": {
        "id": "N07qa_gYgFcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.write.mode(\"overwrite\").saveAsTable(\"employees\")\n",
        "attendance_df.write.mode(\"overwrite\").saveAsTable(\"attendance\")\n",
        "bonuses_df.write.mode(\"overwrite\").saveAsTable(\"bonuses\")"
      ],
      "metadata": {
        "id": "va1QLQl2gGBY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL – Top paid employee in each department"
      ],
      "metadata": {
        "id": "CAQCjewhgKTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT Department, Name, Salary\n",
        "    FROM (\n",
        "        SELECT *, RANK() OVER (PARTITION BY Department ORDER BY Salary DESC) as rnk\n",
        "        FROM employees\n",
        "    ) ranked\n",
        "    WHERE rnk = 1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3fQgtfDgOcz",
        "outputId": "32b55b59-36c9-442c-daa3-f3834bfb042c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+------+\n",
            "| Department| Name|Salary|\n",
            "+-----------+-----+------+\n",
            "|Engineering|  Raj| 80000|\n",
            "|         HR|Anita| 55000|\n",
            "|  Marketing|Aamir| 60000|\n",
            "+-----------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL – Attendance rate by department"
      ],
      "metadata": {
        "id": "rvxpa4WngSMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT e.Department,\n",
        "           ROUND(100 * SUM(CASE WHEN a.Status = 'Present' THEN 1 ELSE 0 END) / COUNT(*), 2) AS AttendanceRate\n",
        "    FROM employees e\n",
        "    JOIN attendance a ON e.EmpID = a.EmpID\n",
        "    GROUP BY e.Department\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovXFxEuXgUnY",
        "outputId": "9b24095f-62c2-48ee-f27b-e002ec29a66b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "| Department|AttendanceRate|\n",
            "+-----------+--------------+\n",
            "|Engineering|          75.0|\n",
            "|         HR|         100.0|\n",
            "|  Marketing|           0.0|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL – Employees joined after 2021 with salary > 70,000"
      ],
      "metadata": {
        "id": "isgakAF7gXVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT EmpID, Name, JoinDate, Salary\n",
        "    FROM employees\n",
        "    WHERE JoinDate > '2021-12-31' AND Salary > 70000\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jQpOF6AgaLv",
        "outputId": "c4689186-a0e6-4991-fcc3-4d21a9f106c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+\n",
            "|EmpID|  Name|  JoinDate|Salary|\n",
            "+-----+------+----------+------+\n",
            "|    3|Simran|2022-07-10| 75000|\n",
            "+-----+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 8 – Advanced"
      ],
      "metadata": {
        "id": "eSBBijjGgmfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a UDF to classify department as \"Tech\" vs \"Non-Tech\"."
      ],
      "metadata": {
        "id": "R-EcYmxSgncq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def classify_dept(dept):\n",
        "    tech_depts = [\"Engineering\"]\n",
        "    return \"Tech\" if dept in tech_depts else \"Non-Tech\"\n",
        "\n",
        "classify_dept_udf = udf(classify_dept, StringType())\n",
        "\n",
        "employees_df = employees_df.withColumn(\"DeptType\", classify_dept_udf(col(\"Department\")))\n",
        "\n",
        "employees_df.select(\"Department\", \"DeptType\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UM6ZSRrgm4j",
        "outputId": "d7b35a06-7d31-4b1d-f0ac-644818c7be9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+\n",
            "| Department|DeptType|\n",
            "+-----------+--------+\n",
            "|         HR|Non-Tech|\n",
            "|Engineering|    Tech|\n",
            "|Engineering|    Tech|\n",
            "|  Marketing|Non-Tech|\n",
            "|         HR|Non-Tech|\n",
            "+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a view emp_attendance_summary ."
      ],
      "metadata": {
        "id": "-nJeknR4g1_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attendance_summary_df = attendance_df.groupBy(\"EmpID\") \\\n",
        "    .pivot(\"Status\", [\"Present\", \"Absent\"]) \\\n",
        "    .count() \\\n",
        "    .na.fill(0)\n",
        "\n",
        "emp_attendance_summary_df = employees_df.join(attendance_summary_df, \"EmpID\", \"left\")\n",
        "\n",
        "emp_attendance_summary_df.createOrReplaceTempView(\"emp_attendance_summary\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM emp_attendance_summary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlRgJekHg2gH",
        "outputId": "75271673-6fc2-4c7f-e86e-bc875610b0ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+----------+------+---------+-----------+--------+---------+-------+--------+-------+------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|TenureYears|JoinYear|JoinMonth|EmpCode|DeptType|Present|Absent|\n",
            "+-----+------+-----------+----------+------+---------+-----------+--------+---------+-------+--------+-------+------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|       4.11|    2021|        5| EMP001|Non-Tech|      2|     0|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|        1|       5.24|    2020|        3| EMP002|    Tech|      1|     1|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|        1|       2.92|    2022|        7| EMP003|    Tech|      2|     0|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|        1|       5.56|    2019|       11| EMP004|Non-Tech|      0|     2|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|        1|       2.43|    2023|        1| EMP005|Non-Tech|      2|     0|\n",
            "+-----+------+-----------+----------+------+---------+-----------+--------+---------+-------+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save it as Parquet partitioned by Department ."
      ],
      "metadata": {
        "id": "FSIDufe9g-tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_attendance_summary_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"Department\") \\\n",
        "    .parquet(\"/content/drive/MyDrive/pyspark_data/emp_attendance_summary_parquet\")\n"
      ],
      "metadata": {
        "id": "kDu9FFpBg_Ez"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}