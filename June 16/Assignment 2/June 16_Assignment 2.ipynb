{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f3680b-a3e0-491e-911b-4996d53d8d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SubscriptionAnalytics\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4f2419-ac46-4baf-90d7-22d4314b2e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_subs = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"file:/Workspace/Shared/subscriptions.csv\")\n",
    "df_activity = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"file:/Workspace/Shared/user_activity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f6aeb5-a5ac-4445-92fa-5c6ded3ffbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_subs.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/subscriptions\")\n",
    "df_activity.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/user_activity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6443dce6-b03c-413f-8e85-bc484cd7caa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A. Subscription Engagement Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57eec689-3ce8-4d7e-9d6e-a4466019400e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------------------+\n|SubscriptionID|UserID|  engagement_score|\n+--------------+------+------------------+\n|        SUB001|  U001|2.9670329670329667|\n|        SUB002|  U002|               4.0|\n|        SUB003|  U003| 2.934782608695652|\n|        SUB004|  U001|11.868131868131867|\n|        SUB005|  U004| 1.978021978021978|\n|        SUB006|  U005|               0.0|\n|        SUB007|  U006|               0.0|\n|        SUB008|  U006|               0.0|\n|        SUB009|  U006|               0.0|\n+--------------+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, count, col\n",
    "\n",
    "df_subs = df_subs.withColumn(\"active_days\", datediff(\"EndDate\", \"StartDate\"))\n",
    "\n",
    "events_per_user = df_activity.groupBy(\"UserID\").agg(count(\"*\").alias(\"events_per_user\"))\n",
    "\n",
    "df_engagement = df_subs.join(events_per_user, on=\"UserID\", how=\"left\").fillna(0)\n",
    "df_engagement = df_engagement.withColumn(\n",
    "    \"engagement_score\",\n",
    "    (col(\"events_per_user\") / col(\"active_days\")) * col(\"PriceUSD\")\n",
    ")\n",
    "df_engagement.select(\"SubscriptionID\", \"UserID\", \"engagement_score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a409f8d8-bc2d-44cd-bf17-c8d5b826c013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "B. Anomaly Detection via SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa4e508-aea1-4a18-bb31-9378afc7aab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Subscription inactive but recent activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd8e1af-b25b-449d-bcf1-3d2fc3a1d923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inactive Subscriptions but Recent Activity:\n+------+\n|UserID|\n+------+\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_activity = df_activity.withColumn(\"EventTime\", col(\"EventTime\").cast(\"timestamp\"))\n",
    "\n",
    "df_anomaly_1 = df_subs.filter(~col(\"IsActive\")) \\\n",
    "    .join(df_activity, \"UserID\") \\\n",
    "    .filter(col(\"EventTime\") > col(\"EndDate\")) \\\n",
    "    .select(\"UserID\").distinct()\n",
    "\n",
    "print(\"Inactive Subscriptions but Recent Activity:\")\n",
    "df_anomaly_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e787037e-e68b-4a91-9ab4-af6086fc2798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "AutoRenew is true but no events in 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b03a6d1-da6f-47b8-983e-0aa29a33fc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AutoRenew True but No Activity in 30 Days:\n+------+\n|UserID|\n+------+\n|  U001|\n|  U006|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as max_, current_date, date_sub\n",
    "\n",
    "latest_events = df_activity.groupBy(\"UserID\").agg(max_(\"EventTime\").alias(\"last_event\"))\n",
    "\n",
    "df_anomaly_2 = df_subs.filter(col(\"AutoRenew\") == True) \\\n",
    "    .join(latest_events, \"UserID\", \"left\") \\\n",
    "    .filter((col(\"last_event\").isNull()) | (col(\"last_event\") < date_sub(current_date(), 30))) \\\n",
    "    .select(\"UserID\").distinct()\n",
    "\n",
    "print(\" AutoRenew True but No Activity in 30 Days:\")\n",
    "df_anomaly_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63f821ba-cf11-4769-b393-c0c1c6692178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "C. Delta Lake + Merge Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f8b35f-de58-4023-a087-0967d676b439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merge fix applied to Pro subscriptions in March.\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "delta_subs = DeltaTable.forPath(spark, \"/tmp/delta/subscriptions\")\n",
    "\n",
    "fix_df = spark.read.format(\"delta\").load(\"/tmp/delta/subscriptions\") \\\n",
    "    .filter((col(\"PlanType\") == \"Pro\") & \n",
    "            (col(\"StartDate\") >= \"2024-03-01\") & \n",
    "            (col(\"StartDate\") <= \"2024-03-31\")) \\\n",
    "    .withColumn(\"PriceUSD\", col(\"PriceUSD\") + lit(5))\n",
    "\n",
    "delta_subs.alias(\"target\").merge(\n",
    "    fix_df.alias(\"source\"),\n",
    "    \"target.SubscriptionID = source.SubscriptionID\"\n",
    ").whenMatchedUpdateAll().execute()\n",
    "\n",
    "print(\" Merge fix applied to Pro subscriptions in March.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07897162-7455-40a1-9822-1180521bd83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "D. Time Travel Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45735f7-f9c9-478e-b9cd-6bf6d5d63c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                 |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|8      |2025-06-16 10:27:38|4028198190791787|azuser3553_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                      |NULL|{1368999269203464}|0611-043506-43vn1hs6|7          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4363, p25FileSize -> 2307, numDeletionVectorsRemoved -> 1, minFileSize -> 2307, numAddedFiles -> 1, maxFileSize -> 2307, p75FileSize -> 2307, p50FileSize -> 2307, numAddedBytes -> 2307}                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|7      |2025-06-16 10:27:36|4028198190791787|azuser3553_mml.local@techademy.com|MERGE    |{predicate -> [\"(SubscriptionID#6777 = SubscriptionID#6793)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> []}|NULL|{1368999269203464}|0611-043506-43vn1hs6|6          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2063, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 1575, materializeSourceTimeMs -> 3, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 700, numTargetRowsUpdated -> 3, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 847}  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|6      |2025-06-16 10:26:59|4028198190791787|azuser3553_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                        |NULL|{1368999269203464}|0611-043506-43vn1hs6|5          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 9, numOutputBytes -> 2300}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|5      |2025-06-16 10:20:39|4028198190791787|azuser3553_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                        |NULL|{1368999269203464}|0611-043506-43vn1hs6|4          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2230}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2025-06-16 10:15:37|4028198190791787|azuser3553_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                      |NULL|{1368999269203464}|0611-043506-43vn1hs6|3          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4184, p25FileSize -> 2230, numDeletionVectorsRemoved -> 1, minFileSize -> 2230, numAddedFiles -> 1, maxFileSize -> 2230, p75FileSize -> 2230, p50FileSize -> 2230, numAddedBytes -> 2230}                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2025-06-16 10:15:34|4028198190791787|azuser3553_mml.local@techademy.com|MERGE    |{predicate -> [\"(SubscriptionID#2555 = SubscriptionID#2571)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> []}|NULL|{1368999269203464}|0611-043506-43vn1hs6|2          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1950, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2368, materializeSourceTimeMs -> 7, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1073, numTargetRowsUpdated -> 1, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1248}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2025-06-16 10:13:33|4028198190791787|azuser3553_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                      |NULL|{1368999269203464}|0611-043506-43vn1hs6|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4247, p25FileSize -> 2234, numDeletionVectorsRemoved -> 1, minFileSize -> 2234, numAddedFiles -> 1, maxFileSize -> 2234, p75FileSize -> 2234, p50FileSize -> 2234, numAddedBytes -> 2234}                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-16 10:13:29|4028198190791787|azuser3553_mml.local@techademy.com|UPDATE   |{predicate -> [\"(((PlanType#1276 = Pro) AND (StartDate#1277 >= 2024-03-01)) AND (StartDate#1277 <= 2024-03-31))\"]}                                                                                  |NULL|{1368999269203464}|0611-043506-43vn1hs6|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 3468, numDeletionVectorsUpdated -> 0, scanTimeMs -> 1819, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 2022, rewriteTimeMs -> 1615}                                                                                                                                                                                                                                                                                                                                                       |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-16 10:07:50|4028198190791787|azuser3553_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                        |NULL|{1368999269203464}|0611-043506-43vn1hs6|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2225}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n===== BEFORE Billing Fix (Version 0) =====\n+--------------+--------+----------+--------+\n|SubscriptionID|PlanType| StartDate|PriceUSD|\n+--------------+--------+----------+--------+\n|        SUB003|     Pro|2024-03-10|    90.0|\n+--------------+--------+----------+--------+\n\n===== AFTER Billing Fix (Current Version) =====\n+--------------+--------+----------+--------+\n|SubscriptionID|PlanType| StartDate|PriceUSD|\n+--------------+--------+----------+--------+\n|        SUB003|     Pro|2024-03-10|    95.0|\n|        SUB006|     Pro|2024-03-05|    95.0|\n|        SUB008|     Pro|2024-03-11|    65.0|\n+--------------+--------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta/subscriptions`\").show(truncate=False)\n",
    "\n",
    "print(\"===== BEFORE Billing Fix (Version 0) =====\")\n",
    "df_before = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta/subscriptions\")\n",
    "df_before.filter(\n",
    "    (col(\"PlanType\") == \"Pro\") &\n",
    "    (col(\"StartDate\") >= \"2024-03-01\") &\n",
    "    (col(\"StartDate\") <= \"2024-03-31\")\n",
    ").select(\"SubscriptionID\", \"PlanType\", \"StartDate\", \"PriceUSD\").show()\n",
    "\n",
    "print(\"===== AFTER Billing Fix (Current Version) =====\")\n",
    "df_after = spark.read.format(\"delta\").load(\"/tmp/delta/subscriptions\")\n",
    "df_after.filter(\n",
    "    (col(\"PlanType\") == \"Pro\") &\n",
    "    (col(\"StartDate\") >= \"2024-03-01\") &\n",
    "    (col(\"StartDate\") <= \"2024-03-31\")\n",
    ").select(\"SubscriptionID\", \"PlanType\", \"StartDate\", \"PriceUSD\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "718390ae-a20d-45ec-a46b-68a21041f6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "E. Build Tier Migration Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23976d88-ca4d-4ef7-858b-6d9a1b74db79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------+\n|UserID|      migration_path| StartDate|PlanType|\n+------+--------------------+----------+--------+\n|  U006|Basic → Pro → Pre...|2024-05-12| Premium|\n+------+--------------------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, when, concat_ws\n",
    "\n",
    "df_subs = spark.read.format(\"delta\").load(\"/tmp/delta/subscriptions\")\n",
    "\n",
    "user_window = Window.partitionBy(\"UserID\").orderBy(\"StartDate\")\n",
    "\n",
    "df_with_lags = df_subs.withColumn(\"prev_plan\", lag(\"PlanType\", 1).over(user_window)) \\\n",
    "                      .withColumn(\"prev2_plan\", lag(\"PlanType\", 2).over(user_window))\n",
    "\n",
    "df_migration = df_with_lags.withColumn(\n",
    "    \"migration_path\",\n",
    "    concat_ws(\" → \", col(\"prev2_plan\"), col(\"prev_plan\"), col(\"PlanType\"))\n",
    ")\n",
    "\n",
    "df_upgraded = df_migration.filter(col(\"migration_path\") == \"Basic → Pro → Premium\")\n",
    "\n",
    "df_upgraded.select(\"UserID\", \"migration_path\", \"StartDate\", \"PlanType\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "663720db-e321-4c7b-8f4b-71d4c454d2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "F. Power Users Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f29e60-e817-41bb-802c-4f96a9cae31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----------+\n|UserID|unique_features|login_count|\n+------+---------------+-----------+\n|  U004|              3|          3|\n|  U001|              4|          3|\n+------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, count, when\n",
    "\n",
    "df_activity = spark.read.format(\"delta\").load(\"/tmp/delta/user_activity\")\n",
    "\n",
    "df_logins = df_activity.filter(col(\"EventType\") == \"login\")\n",
    "\n",
    "df_power = df_activity.groupBy(\"UserID\").agg(\n",
    "    countDistinct(\"FeatureUsed\").alias(\"unique_features\"),\n",
    "    count(when(col(\"EventType\") == \"login\", True)).alias(\"login_count\")\n",
    ")\n",
    "\n",
    "df_power_users = df_power.filter(\n",
    "    (col(\"unique_features\") >= 2) & (col(\"login_count\") >= 3)\n",
    ")\n",
    "\n",
    "df_power_users.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/power_users\")\n",
    "\n",
    "df_power_users.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb3e0e9-38ef-4471-b458-e437ad04230a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "G. Session Replay View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7eb420-3ae5-4c83-aa51-791c5ff73a30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+------------------------+\n|UserID|login_time         |logout_time        |session_duration_minutes|\n+------+-------------------+-------------------+------------------------+\n|U001  |2024-04-07 10:22:00|2024-04-07 10:50:00|28.0                    |\n|U001  |2024-04-08 09:00:00|2024-04-08 09:45:00|45.0                    |\n|U001  |2024-04-09 08:00:00|2024-04-09 08:20:00|20.0                    |\n|U002  |2024-04-08 11:10:00|2024-04-08 11:25:00|15.0                    |\n|U003  |2024-04-09 09:45:00|2024-04-09 10:15:00|30.0                    |\n|U004  |2024-04-11 12:00:00|2024-04-11 12:45:00|45.0                    |\n+------+-------------------+-------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lead, to_timestamp, unix_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_activity = spark.read.format(\"delta\").load(\"/tmp/delta/user_activity\")\n",
    "\n",
    "df_activity = df_activity.withColumn(\"EventTime\", to_timestamp(\"EventTime\"))\n",
    "\n",
    "df_filtered = df_activity.filter(col(\"EventType\").isin(\"login\", \"logout\"))\n",
    "\n",
    "user_window = Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "\n",
    "df_sessions = df_filtered.withColumn(\"next_event\", lead(\"EventType\").over(user_window)) \\\n",
    "                         .withColumn(\"next_time\", lead(\"EventTime\").over(user_window))\n",
    "\n",
    "df_sessions_matched = df_sessions.filter((col(\"EventType\") == \"login\") & (col(\"next_event\") == \"logout\"))\n",
    "\n",
    "df_sessions_result = df_sessions_matched.withColumn(\n",
    "    \"session_duration_minutes\",\n",
    "    (unix_timestamp(\"next_time\") - unix_timestamp(\"EventTime\")) / 60\n",
    ").select(\n",
    "    \"UserID\", \n",
    "    col(\"EventTime\").alias(\"login_time\"), \n",
    "    col(\"next_time\").alias(\"logout_time\"), \n",
    "    \"session_duration_minutes\"\n",
    ")\n",
    "\n",
    "df_sessions_result.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/user_sessions\")\n",
    "\n",
    "df_sessions_result.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June 16_Assignment 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}