{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8b4e63-e4a5-49f0-97c4-d9289070e1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, max, min, when, current_date, datediff, lit\n",
    "from pyspark.sql.functions import rand, expr, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, sum as _sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AdvancedEmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Dataset 1: employee_data\n",
    "data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Dataset 2: performance_data\n",
    "performance = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3abf6ae4-c647-4f85-908a-217c06f1d580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GroupBy and Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0bb0f69-5414-4b65-9b08-e9705c85a35a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Get the average salary by department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac51110-1aa8-45a5-b3f9-c2a376ca9612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n| Department|AvgSalary|\n+-----------+---------+\n|         HR|  52500.0|\n|Engineering|  65000.0|\n|  Marketing|  46500.0|\n+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_emp.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "438062cc-cbf4-4711-8da6-d284a6c35e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Count of employees per department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1ba007a-56b0-4ec0-bbe3-edc58ec9b111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n| Department|EmployeeCount|\n+-----------+-------------+\n|         HR|            2|\n|Engineering|            3|\n|  Marketing|            2|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_emp.groupBy(\"Department\").agg(count(\"*\").alias(\"EmployeeCount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f787df6e-cf76-4d4f-a717-ea5c4b03aa53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Maximum and minimum salary in Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93519f53-9f36-4174-8262-f7ac78b6400e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n|MaxSalary|MinSalary|\n+---------+---------+\n|    70000|    60000|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_emp.filter(col(\"Department\") == \"Engineering\").agg(\n",
    "    max(\"Salary\").alias(\"MaxSalary\"),\n",
    "    min(\"Salary\").alias(\"MinSalary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c87277-2440-45eb-8241-54f3210f2a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join and Combine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79dee69-2062-46bf-8cf9-7b647beb3813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Perform an inner join between employee_data and performance_data on Name ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951cc5e2-d3ab-475d-9bfe-362463f8b28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_emp.join(df_perf, on=\"Name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d4a7b58-7bf6-4610-bce4-73a8ec7bda39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Show each employeeâ€™s salary and performance rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9332d3-e7e1-4c94-8bce-1de8825785bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n|Ananya| 52000|   4.5|\n|Fatima| 45000|   3.9|\n| Karan| 53000|   4.1|\n|Naveen| 70000|   4.7|\n| Priya| 60000|   4.3|\n| Rahul| 65000|   4.9|\n|  Zoya| 48000|   3.8|\n+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.select(\"Name\", \"Salary\", \"Rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04abc9cd-8d1f-4613-9cb1-a9fe0feaaba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Filter employees with rating > 4.5 and salary > 60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d3be53-3f13-4e8e-80fd-9bf1057fc1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Naveen|Engineering| 70000|2023|   4.7|\n| Rahul|Engineering| 65000|2023|   4.9|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.filter((col(\"Rating\") > 4.5) & (col(\"Salary\") > 60000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966a0086-1387-4fa8-b997-5caa30589e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Window & Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f20e8e-5279-4e55-931c-95ca1b610f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Rank employees by salary department-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8bd2654-0821-4769-87a1-52a08589b2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+\n|  Name| Department|Salary|SalaryRank|\n+------+-----------+------+----------+\n|Naveen|Engineering| 70000|         1|\n| Rahul|Engineering| 65000|         2|\n| Priya|Engineering| 60000|         3|\n| Karan|         HR| 53000|         1|\n|Ananya|         HR| 52000|         2|\n|  Zoya|  Marketing| 48000|         1|\n|Fatima|  Marketing| 45000|         2|\n+------+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "df_ranked = df_emp.withColumn(\"SalaryRank\", rank().over(windowSpec))\n",
    "df_ranked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784a3cef-231c-4d08-974a-7ee39d3115fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Calculate cumulative salary in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a76ec3-1ac7-459d-9c63-9d2a1461da35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------------+\n|  Name| Department|Salary|CumulativeSalary|\n+------+-----------+------+----------------+\n|Naveen|Engineering| 70000|           70000|\n| Rahul|Engineering| 65000|          135000|\n| Priya|Engineering| 60000|          195000|\n| Karan|         HR| 53000|           53000|\n|Ananya|         HR| 52000|          105000|\n|  Zoya|  Marketing| 48000|           48000|\n|Fatima|  Marketing| 45000|           93000|\n+------+-----------+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_cumulative = df_emp.withColumn(\"CumulativeSalary\", _sum(\"Salary\").over(windowSpec.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "df_cumulative.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc982d0-4029-4f23-9cea-db0d11c4902d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be400656-854a-427e-bd7f-3ec97f302284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Add a new column JoinDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06294cfd-07d7-40bd-a2d2-d9a974764e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+\n|  Name| Department|Salary|  JoinDate|\n+------+-----------+------+----------+\n|Ananya|         HR| 52000|2020-10-04|\n| Rahul|Engineering| 65000|2023-01-17|\n| Priya|Engineering| 60000|2023-08-28|\n|  Zoya|  Marketing| 48000|2021-08-05|\n| Karan|         HR| 53000|2021-01-28|\n|Naveen|Engineering| 70000|2021-10-18|\n|Fatima|  Marketing| 45000|2020-11-08|\n+------+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_dates = df_emp.withColumn(\"JoinDate\", expr(\"date_add(to_date('2020-01-01'), cast(rand() * 1460 as int))\"))\n",
    "df_dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "088bd499-3224-4eb1-86b9-44cee3a73eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Add column YearsWithCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab1dfec-3d65-4392-a894-0d2f18c77e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+----------------+\n|  Name| Department|Salary|  JoinDate|YearsWithCompany|\n+------+-----------+------+----------+----------------+\n|Ananya|         HR| 52000|2020-10-04|               4|\n| Rahul|Engineering| 65000|2023-01-17|               2|\n| Priya|Engineering| 60000|2023-08-28|               1|\n|  Zoya|  Marketing| 48000|2021-08-05|               3|\n| Karan|         HR| 53000|2021-01-28|               4|\n|Naveen|Engineering| 70000|2021-10-18|               3|\n|Fatima|  Marketing| 45000|2020-11-08|               4|\n+------+-----------+------+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_tenure = df_dates.withColumn(\"YearsWithCompany\", (datediff(current_date(), col(\"JoinDate\")) / 365).cast(\"int\"))\n",
    "df_tenure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c16b87-fff0-4dc5-adfa-696bbeb9a817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Writing to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46d6807d-8ea0-40c9-b02b-e8f0bf0f8800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. Write the full employee DataFrame to CSV with headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a306a4-8890-4953-8275-dede251f7610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp.write.mode(\"overwrite\").option(\"header\", True).csv(\"/tmp/employee_data_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d71c7e-4088-4770-8aea-47291d6f4961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. Save the joined DataFrame to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347d0b6e-6dbb-4e21-8fdd-0d90b2ec239e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined.write.mode(\"overwrite\").parquet(\"/tmp/employee_performance_parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June 11_Ex 2 Aggregation and grouping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}