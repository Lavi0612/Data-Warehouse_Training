{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4a942b-1589-448c-b909-433432ca731a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, max, min, when, current_date, datediff, lit\n",
    "from pyspark.sql.functions import rand, expr, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, sum as _sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AdvancedEmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Dataset 1: employee_data\n",
    "data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Dataset 2: performance_data\n",
    "performance = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)\n",
    "\n",
    "# dataset 3: project_data\n",
    "project_data = [\n",
    "    (\"Ananya\", \"HR Portal\", 120),\n",
    "    (\"Rahul\", \"Data Platform\", 200),\n",
    "    (\"Priya\", \"Data Platform\", 180),\n",
    "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
    "    (\"Karan\", \"HR Portal\", 130),\n",
    "    (\"Naveen\", \"ML Pipeline\", 220),\n",
    "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b401cb6b-52d0-4f81-b530-22ff3ec757cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Joins and Advanced Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5c6792-d266-433b-a892-7ecaa19e94e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Join employee_data , performance_data , and project_data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33df19fa-89a2-4d04-a0e5-4901e2c25d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_all = df_emp \\\n",
    "    .join(df_perf, on=\"Name\", how=\"left\") \\\n",
    "    .join(df_proj, on=\"Name\", how=\"left\")\n",
    "df_all.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be333741-628f-419a-80f0-f437404096d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Compute total hours worked per department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3a0b68-6916-40dd-84fd-0b5928dabb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n| Department|TotalHours|\n+-----------+----------+\n|         HR|       250|\n|Engineering|       600|\n|  Marketing|       190|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_all.groupBy(\"Department\").agg({\"HoursWorked\": \"sum\"}).withColumnRenamed(\"sum(HoursWorked)\", \"TotalHours\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9f7390b-0efc-412b-b925-fa0bceedf67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Compute average rating per project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9ae945-5c71-4b62-b40e-f11a85b5767b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|         Project|         AvgRating|\n+----------------+------------------+\n|   Data Platform|               4.6|\n|       HR Portal|               4.3|\n|     ML Pipeline|               4.7|\n|Campaign Tracker|3.8499999999999996|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_all.groupBy(\"Project\").agg({\"Rating\": \"avg\"}).withColumnRenamed(\"avg(Rating)\", \"AvgRating\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f7a5983-331b-40bc-b9a3-205582a9920d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f9f4c0b-df61-4fd9-bb18-61eb32f40e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Add a row to performance_data with a None rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bf6ec0-bc5e-4404-9bf0-0ae0b285236c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n|  John|2023|  NULL|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "schema = StructType([StructField(\"Name\", StringType(), True), StructField(\"Year\", IntegerType(), True), StructField(\"Rating\", FloatType(), True)])\n",
    "new_row = spark.createDataFrame([Row(Name=\"John\", Year=2023, Rating=None)], schema=schema)\n",
    "\n",
    "df_perf_with_null = df_perf.union(new_row)\n",
    "df_perf_with_null.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c858c446-0f8b-4339-a877-8750632bed18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Filter rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b625af3-1c6e-43f5-ad4e-4610e31dd81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n|Name|Year|Rating|\n+----+----+------+\n|John|2023|  NULL|\n+----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_perf_with_null.filter(col(\"Rating\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d5b13ab-e015-46ec-b85c-512eeb630f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Replace null ratings with the department average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7016c4fc-91a7-41f3-bb7c-f3e336caee3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n|  Name| Department|FinalRating|\n+------+-----------+-----------+\n|Ananya|         HR|        4.5|\n| Rahul|Engineering|        4.9|\n| Priya|Engineering|        4.3|\n|  Zoya|  Marketing|        3.8|\n| Karan|         HR|        4.1|\n|Naveen|Engineering|        4.7|\n|Fatima|  Marketing|        3.9|\n|  John|       NULL|       NULL|\n+------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_perf_dept = df_perf_with_null.join(df_emp, on=\"Name\", how=\"left\")\n",
    "\n",
    "avg_rating_by_dept = df_perf_dept.groupBy(\"Department\") \\\n",
    "    .agg(avg(\"Rating\").alias(\"DeptAvgRating\"))\n",
    "\n",
    "df_filled = df_perf_dept.join(avg_rating_by_dept, on=\"Department\", how=\"left\") \\\n",
    "    .withColumn(\"FinalRating\", when(col(\"Rating\").isNull(), col(\"DeptAvgRating\")).otherwise(col(\"Rating\")))\n",
    "\n",
    "df_filled.select(\"Name\", \"Department\", \"FinalRating\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771f71e3-b883-4beb-8c13-8b83892f104e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Built-In Functions and UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf680b7-5443-4874-bd14-f2bdcfe565e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Create a column PerformanceCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6079aff1-b11e-48d3-8d75-a4716b1554cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------------+\n|  Name|Year|Rating|PerformanceCategory|\n+------+----+------+-------------------+\n|Ananya|2023|   4.5|               Good|\n| Rahul|2023|   4.9|          Excellent|\n| Priya|2023|   4.3|               Good|\n|  Zoya|2023|   3.8|            Average|\n| Karan|2023|   4.1|               Good|\n|Naveen|2023|   4.7|          Excellent|\n|Fatima|2023|   3.9|            Average|\n+------+----+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_perf_cat = df_perf.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
    "    .when(col(\"Rating\") >= 4.0, \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "df_perf_cat.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71da2331-0c01-4bcb-926a-220ec2ba166b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Create a UDF to assign bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a8e231-9a06-4026-9d57-fd191d434993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----------+-----+\n|  Name|         Project|HoursWorked|Bonus|\n+------+----------------+-----------+-----+\n|Ananya|       HR Portal|        120| 5000|\n| Rahul|   Data Platform|        200| 5000|\n| Priya|   Data Platform|        180| 5000|\n|  Zoya|Campaign Tracker|        100| 5000|\n| Karan|       HR Portal|        130| 5000|\n|Naveen|     ML Pipeline|        220|10000|\n|Fatima|Campaign Tracker|         90| 5000|\n+------+----------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def calculate_bonus(hours):\n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus_udf = udf(calculate_bonus, IntegerType())\n",
    "\n",
    "df_with_bonus = df_proj.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\")))\n",
    "df_with_bonus.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54b31ef-bfba-45a2-943d-15ebcb35ce78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date and Time Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ad4f40-5184-4987-9a26-102ac12c59c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Add a column JoinDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb3c67e-0583-451d-82e6-080c4697a768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+------------+\n|  Name| Department|Salary|  JoinDate|MonthsWorked|\n+------+-----------+------+----------+------------+\n|Ananya|         HR| 52000|2021-06-01|          48|\n| Rahul|Engineering| 65000|2021-06-01|          48|\n| Priya|Engineering| 60000|2021-06-01|          48|\n|  Zoya|  Marketing| 48000|2021-06-01|          48|\n| Karan|         HR| 53000|2021-06-01|          48|\n|Naveen|Engineering| 70000|2021-06-01|          48|\n|Fatima|  Marketing| 45000|2021-06-01|          48|\n+------+-----------+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, months_between\n",
    "\n",
    "df_with_dates = df_emp.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\"))) \\\n",
    "    .withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")).cast(\"int\"))\n",
    "\n",
    "df_with_dates.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "057fab39-f191-4478-a07b-5e490f9f8758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Calculate how many employees joined before 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a40dee-33df-4f9e-9e67-a9bdff052f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_dates.filter(col(\"JoinDate\") < to_date(lit(\"2022-01-01\"))).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b349403-319e-4119-929b-963ab8cd4531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "121926a5-ab44-4a9b-8e00-73e59d86a841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. Create another small team DataFrame and union() it with employee_data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9458bb-2aa3-43de-8e3a-9af7cd7e2fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "extra_employees = [\n",
    "    (\"Meena\", \"HR\", 48000),\n",
    "    (\"Raj\", \"Marketing\", 51000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_extra = spark.createDataFrame(extra_employees, columns)\n",
    "\n",
    "df_union = df_emp.union(df_extra)\n",
    "df_union.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94058ab5-0fe1-4f0c-b960-4b9b653e3081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Saving Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0878c3d2-6bdd-41b7-b3f8-9811e53cd737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. Save the final merged dataset (all 3 joins) as a partitioned Parquet file based\n",
    "on Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c940b29-bfa1-4ae2-9a7e-fd89bf441be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_all.write.mode(\"overwrite\").partitionBy(\"Department\").parquet(\"/tmp/employee_project_performance\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June 11_Ex 3 Combining Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}