{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ae6c81-28a7-4066-ac4a-111ff5f855b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SmartCityTrafficMonitoring\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5884d46e-88d3-498f-9536-cb5b8d5d9439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1 Data Ingestion & Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746ea5bd-5755-458c-b1ec-3aef8df5cfd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TrafficMonitoringSystem\").getOrCreate()\n",
    "\n",
    "traffic_df_inferred = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/traffic_logs.csv\")\n",
    "\n",
    "manual_schema = StructType([\n",
    "    StructField(\"LogID\", StringType(), True),\n",
    "    StructField(\"VehicleID\", StringType(), True),\n",
    "    StructField(\"EntryPoint\", StringType(), True),\n",
    "    StructField(\"ExitPoint\", StringType(), True),\n",
    "    StructField(\"EntryTime\", TimestampType(), True),\n",
    "    StructField(\"ExitTime\", TimestampType(), True),\n",
    "    StructField(\"VehicleType\", StringType(), True),\n",
    "    StructField(\"SpeedKMH\", IntegerType(), True),\n",
    "    StructField(\"TollPaid\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "traffic_df = spark.read.option(\"header\", True).schema(manual_schema).csv(\"file:/Workspace/Shared/traffic_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5222e2-d3f9-44bd-93af-864aee6f0ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2 Derived Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70020fc4-9d82-46c0-bea3-acd1cc91bd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, when\n",
    "\n",
    "#Calculate TripDurationMinutes = ExitTime - EntryTime\n",
    "traffic_df = traffic_df.withColumn(\"TripDurationMinutes\", \n",
    "    (unix_timestamp(\"ExitTime\") - unix_timestamp(\"EntryTime\")) / 60)\n",
    "    \n",
    "#Add IsOverspeed = SpeedKMH > 60\n",
    "traffic_df = traffic_df.withColumn(\"IsOverspeed\", col(\"SpeedKMH\") > 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c5c4ba6-5f22-41fe-a8bd-1511550038b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3 Vehicle Behavior Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67eaa800-eada-4769-b0ce-008e81b0b8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n|VehicleType|AvgSpeed|\n+-----------+--------+\n|      Truck|    45.0|\n|       Bike|    55.0|\n|        Bus|    40.0|\n|        Car|    70.0|\n+-----------+--------+\n\n+----------+---------+\n|EntryPoint|TotalToll|\n+----------+---------+\n|     GateC|     50.0|\n|     GateA|     80.0|\n|     GateB|    170.0|\n+----------+---------+\n\n+---------+-----+\n|ExitPoint|Count|\n+---------+-----+\n|    GateC|    2|\n+---------+-----+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum, count, desc\n",
    "\n",
    "# Avg speed per VehicleType\n",
    "traffic_df.groupBy(\"VehicleType\").agg(avg(\"SpeedKMH\").alias(\"AvgSpeed\")).show()\n",
    "\n",
    "# Total toll collected by EntryPoint\n",
    "traffic_df.groupBy(\"EntryPoint\").agg(sum(\"TollPaid\").alias(\"TotalToll\")).show()\n",
    "\n",
    "# Most used ExitPoint\n",
    "traffic_df.groupBy(\"ExitPoint\").agg(count(\"*\").alias(\"Count\")).orderBy(desc(\"Count\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3704f19-19df-4594-a636-3070b5f499d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4 Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c04695-2db1-411f-850f-f278c8932ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------+---------+-------------------+-------------------+------------+\n|LogID|VehicleID|VehicleType|SpeedKMH|SpeedRank|EntryTime          |ExitTime           |LastExitTime|\n+-----+---------+-----------+--------+---------+-------------------+-------------------+------------+\n|L001 |V001     |Car        |60      |2        |2024-05-01 08:01:00|2024-05-01 08:20:00|NULL        |\n|L002 |V002     |Truck      |45      |1        |2024-05-01 08:10:00|2024-05-01 08:45:00|NULL        |\n|L003 |V003     |Bike       |55      |1        |2024-05-01 09:00:00|2024-05-01 09:18:00|NULL        |\n|L004 |V004     |Car        |80      |1        |2024-05-01 09:15:00|2024-05-01 09:35:00|NULL        |\n|L005 |V005     |Bus        |40      |1        |2024-05-01 10:05:00|2024-05-01 10:40:00|NULL        |\n+-----+---------+-----------+--------+---------+-------------------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, lag\n",
    "\n",
    "# Rank vehicles by speed within VehicleType\n",
    "windowSpec = Window.partitionBy(\"VehicleType\").orderBy(desc(\"SpeedKMH\"))\n",
    "traffic_df = traffic_df.withColumn(\"SpeedRank\", rank().over(windowSpec))\n",
    "\n",
    "# Find last exit time for each vehicle using lag()\n",
    "windowSpec2 = Window.partitionBy(\"VehicleID\").orderBy(\"EntryTime\")\n",
    "traffic_df = traffic_df.withColumn(\"LastExitTime\", lag(\"ExitTime\").over(windowSpec2))\n",
    "\n",
    "traffic_df.select(\n",
    "    \"LogID\", \"VehicleID\", \"VehicleType\", \"SpeedKMH\", \"SpeedRank\",\n",
    "    \"EntryTime\", \"ExitTime\", \"LastExitTime\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "385241ff-9738-42c7-a949-9cb5f1bf254a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5 Session Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17875f67-0033-4373-8d42-f5f2b5416481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+------------+-----------+\n|VehicleID|EntryTime          |ExitTime           |LastExitTime|IdleMinutes|\n+---------+-------------------+-------------------+------------+-----------+\n|V001     |2024-05-01 08:01:00|2024-05-01 08:20:00|NULL        |NULL       |\n|V002     |2024-05-01 08:10:00|2024-05-01 08:45:00|NULL        |NULL       |\n|V003     |2024-05-01 09:00:00|2024-05-01 09:18:00|NULL        |NULL       |\n|V004     |2024-05-01 09:15:00|2024-05-01 09:35:00|NULL        |NULL       |\n|V005     |2024-05-01 10:05:00|2024-05-01 10:40:00|NULL        |NULL       |\n+---------+-------------------+-------------------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import unix_timestamp, col, lag\n",
    "\n",
    "session_window = Window.partitionBy(\"VehicleID\").orderBy(\"EntryTime\")\n",
    "\n",
    "traffic_df = traffic_df.withColumn(\"LastExitTime\", lag(\"ExitTime\").over(session_window))\n",
    "\n",
    "traffic_df = traffic_df.withColumn(\"IdleMinutes\", \n",
    "    (unix_timestamp(\"EntryTime\") - unix_timestamp(\"LastExitTime\")) / 60)\n",
    "\n",
    "traffic_df.select(\n",
    "    \"VehicleID\", \"EntryTime\", \"ExitTime\", \"LastExitTime\", \"IdleMinutes\"\n",
    ").orderBy(\"VehicleID\", \"EntryTime\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fba5db6-00f8-41bc-9424-fd9b939f9189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6 Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd82109-651c-41f6-bab1-9530449e3807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------+---------+--------+\n|VehicleID|SpeedKMH|TripDurationMinutes|EntryTime|ExitTime|\n+---------+--------+-------------------+---------+--------+\n+---------+--------+-------------------+---------+--------+\n\n+---------+-------------------+--------+----------+---------+\n|VehicleID|TripDurationMinutes|TollPaid|EntryPoint|ExitPoint|\n+---------+-------------------+--------+----------+---------+\n+---------+-------------------+--------+----------+---------+\n\n+---------+----------+---------+-------------------+-------------------+\n|VehicleID|EntryPoint|ExitPoint|EntryTime          |ExitTime           |\n+---------+----------+---------+-------------------+-------------------+\n|V005     |GateB     |GateA    |2024-05-01 10:05:00|2024-05-01 10:40:00|\n+---------+----------+---------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Identify vehicles with speed > 70 and TripDuration < 10 minutes\n",
    "anomaly_fast_short = traffic_df.filter(\n",
    "    (col(\"SpeedKMH\") > 70) & (col(\"TripDurationMinutes\") < 10)\n",
    ")\n",
    "\n",
    "anomaly_fast_short.select(\n",
    "    \"VehicleID\", \"SpeedKMH\", \"TripDurationMinutes\", \"EntryTime\", \"ExitTime\"\n",
    ").show(truncate=False)\n",
    "\n",
    "#Vehicles that paid less toll for longer trips\n",
    "anomaly_low_toll_long_trip = traffic_df.filter(\n",
    "    (col(\"TripDurationMinutes\") > 25) & (col(\"TollPaid\") < 50)\n",
    ")\n",
    "\n",
    "anomaly_low_toll_long_trip.select(\n",
    "    \"VehicleID\", \"TripDurationMinutes\", \"TollPaid\", \"EntryPoint\", \"ExitPoint\"\n",
    ").show(truncate=False)\n",
    "\n",
    "#Suspicious backtracking (ExitPoint earlier than EntryPoint)\n",
    "anomaly_backtrack = traffic_df.filter(\n",
    "    col(\"ExitPoint\") < col(\"EntryPoint\")\n",
    ")\n",
    "\n",
    "anomaly_backtrack.select(\n",
    "    \"VehicleID\", \"EntryPoint\", \"ExitPoint\", \"EntryTime\", \"ExitTime\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b616120-f8bb-41e3-bf78-1764287e1cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7 Join with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9126eb34-b3ce-45ee-abe0-4c2e5cdb549c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n|RegisteredCity|TotalTrips|\n+--------------+----------+\n|     Bangalore|         1|\n|          Pune|         1|\n|         Delhi|         1|\n|       Chennai|         1|\n|        Mumbai|         1|\n+--------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "vehicle_registry_schema = StructType([\n",
    "    StructField(\"VehicleID\", StringType(), True),\n",
    "    StructField(\"OwnerName\", StringType(), True),\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"RegisteredCity\", StringType(), True),\n",
    "])\n",
    "\n",
    "registry_df = spark.read.option(\"header\", True).schema(vehicle_registry_schema).csv(\"file:/Workspace/Shared/vehicle_registry.csv\")\n",
    "\n",
    "# Join and group trips by RegisteredCity\n",
    "enriched_df = traffic_df.join(registry_df, \"VehicleID\", \"left\")\n",
    "enriched_df.groupBy(\"RegisteredCity\").agg(count(\"*\").alias(\"TotalTrips\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "047bde76-2038-4a5c-aca4-f13bc64aa1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8 Delta Lake Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3ac6ce-2532-49b5-a75e-9bb99be098cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      3|2025-06-19 05:02:13|4028198190791787|azuser3553_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{1052067078041120}|0611-043506-43vn1hs6|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      2|2025-06-19 05:02:11|4028198190791787|azuser3553_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{1052067078041120}|0611-043506-43vn1hs6|          1|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2025-06-19 05:02:08|4028198190791787|azuser3553_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{1052067078041120}|0611-043506-43vn1hs6|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2025-06-19 05:01:58|4028198190791787|azuser3553_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{1052067078041120}|0611-043506-43vn1hs6|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+---------+-----+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+------------+-----------+---------+-----------+--------------+\n|VehicleID|LogID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|SpeedRank|LastExitTime|IdleMinutes|OwnerName|      Model|RegisteredCity|\n+---------+-----+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+------------+-----------+---------+-----------+--------------+\n|     V001| L001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|    50.0|               19.0|      false|        2|        NULL|       NULL|     Anil|Hyundai i20|         Delhi|\n|     V002| L002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|   100.0|               35.0|      false|        1|        NULL|       NULL|   Rakesh| Tata Truck|       Chennai|\n|     V003| L003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|    30.0|               18.0|      false|        1|        NULL|       NULL|     Sana| Yamaha R15|        Mumbai|\n|     V004| L004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|    50.0|               20.0|       true|        1|        NULL|       NULL|     Neha| Honda City|     Bangalore|\n|     V005| L005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|    70.0|               35.0|      false|        1|        NULL|       NULL|     Zoya|  Volvo Bus|          Pune|\n+---------+-----+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+------------+-----------+---------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Save traffic_logs as Delta Table\n",
    "enriched_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/traffic_logs_delta\")\n",
    "\n",
    "delta_df = DeltaTable.forPath(spark, \"/tmp/traffic_logs_delta\")\n",
    "\n",
    "# Apply MERGE INTO to update toll rates for all Bikes\n",
    "delta_df.alias(\"target\").merge(\n",
    "    enriched_df.filter(col(\"VehicleType\") == \"Bike\").alias(\"source\"),\n",
    "    \"target.LogID = source.LogID\"\n",
    ").whenMatchedUpdate(set={\"TollPaid\": \"35\"}).execute()\n",
    "\n",
    "# Delete trips longer than 60 minutes\n",
    "delta_df.delete(\"TripDurationMinutes > 60\")\n",
    "\n",
    "# Use DESCRIBE HISTORY and VERSION AS OF\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/tmp/traffic_logs_delta`\").show()\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/traffic_logs_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "490be72b-e9d9-4f44-8d7f-9ac6481498cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9 Advanced Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf75c8d-b9dd-4d89-8245-ac5e04a7f1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n|VehicleID|TripDate|count|\n+---------+--------+-----+\n+---------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# when/otherwise : Tag trip type\n",
    "enriched_df = enriched_df.withColumn(\"TripType\", \n",
    "    when(col(\"TripDurationMinutes\") < 15, \"Short\")\n",
    "    .when(col(\"TripDurationMinutes\") <= 30, \"Medium\")\n",
    "    .otherwise(\"Long\"))\n",
    "\n",
    "#Flag vehicles with more than 3 trips in a day\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "daily_trips = enriched_df.withColumn(\"TripDate\", to_date(\"EntryTime\"))\n",
    "frequent_vehicles = daily_trips.groupBy(\"VehicleID\", \"TripDate\").count().filter(\"count > 3\")\n",
    "frequent_vehicles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2158ac3-3eb7-4fa3-8d65-897dd0b819e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10 Export & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66635a63-3f64-4fc7-9e78-a2fcb1cedbb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+\n|VehicleType|ExitPoint|TotalToll|\n+-----------+---------+---------+\n|      Truck|    GateC|    100.0|\n|        Car|    GateC|     50.0|\n|        Car|    GateD|     50.0|\n|       Bike|    GateD|     30.0|\n|        Bus|    GateA|     70.0|\n+-----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Parquet partitioned by VehicleType\n",
    "enriched_df.write.partitionBy(\"VehicleType\").mode(\"overwrite\").parquet(\"output/vehicle_partitioned\")\n",
    "\n",
    "#CSV for dashboards\n",
    "enriched_df.write.mode(\"overwrite\").csv(\"output/traffic_summary_csv\", header=True)\n",
    "\n",
    "# Create summary SQL View: total toll by VehicleType + ExitPoint\n",
    "enriched_df.createOrReplaceTempView(\"traffic_view\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT VehicleType, ExitPoint, SUM(TollPaid) AS TotalToll\n",
    "    FROM traffic_view\n",
    "    GROUP BY VehicleType, ExitPoint\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June 19_Assessment(Set1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}