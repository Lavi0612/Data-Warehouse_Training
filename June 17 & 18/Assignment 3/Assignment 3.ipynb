{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab64c9e5-e785-440a-9b70-21586ecbdab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EmployeeTimesheet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e7b4d7-8ab0-41ce-bf6f-719e95443f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Ingestion & Schema Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60fc8ab-14d6-4fdb-bde0-404eb9739190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Load the CSV using inferred schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303cf28a-0284-40d1-8afd-2f97685be622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_inferred = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///Workspace/Shared/employee_timesheet.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0aff97f-42c2-4f6d-aa05-5f1295c3adfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Load the same file with schema explicitly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d7583a-b9a5-4364-b87f-c09f54b29b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Project\", StringType(), True),\n",
    "    StructField(\"WorkHours\", IntegerType(), True),\n",
    "    StructField(\"WorkDate\", DateType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Mode\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_explicit = spark.read.option(\"header\", True).schema(schema).csv(\"file:///Workspace/Shared/employee_timesheet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82dc9d22-b5cb-42a3-b6d3-b26c250087fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Add a new column Weekday extracted from WorkDate ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c71744-460f-4985-b3af-296191225fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_explicit.withColumn(\"Weekday\", date_format(\"WorkDate\", \"EEEE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b66cacf-960f-4cc4-b320-f60a5cda9ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aggregations & Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "787885ce-4a40-4f9e-92ac-cb57d0314101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Calculate total work hours by employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3df834e-fd43-419d-a486-b4258edb8177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n|EmployeeID| Name|TotalHours|\n+----------+-----+----------+\n|      E103| John|         5|\n|      E104|Meena|         6|\n|      E102|  Raj|        15|\n|      E101|Anita|        17|\n+----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76a0bc4-c973-448f-9640-a769bd1146dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Calculate average work hours per department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd18f74-bdcd-408f-9735-edacdc209979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|Department|         AvgHours|\n+----------+-----------------+\n|        HR|              7.5|\n|   Finance|              5.0|\n|        IT|7.666666666666667|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AvgHours\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6eb621e-9c69-450a-9607-a97080a218ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Get top 2 employees by total hours using window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64117a89-bf05-486f-85cd-b34f4241bada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----+\n|EmployeeID| Name|TotalHours|Rank|\n+----------+-----+----------+----+\n|      E101|Anita|        17|   1|\n|      E102|  Raj|        15|   2|\n+----------+-----+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col(\"TotalHours\").desc())\n",
    "df_total = df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\"))\n",
    "df_total.withColumn(\"Rank\", rank().over(window_spec)).filter(\"Rank <= 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0af1cc59-5313-4672-a88b-16edd36f45b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e9d605-efd9-4d94-8f1a-2c0172f79283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Filter entries where WorkDate falls on a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d00add-dafc-4844-b033-413f186da642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-------+---------+----------+--------+------+--------+\n|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location|  Mode| Weekday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|Saturday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Weekday\").isin([\"Saturday\", \"Sunday\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2d0a31-5f3c-43c3-9078-b3b037ec896a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Calculate running total of hours per employee using window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b076b9-718d-40f4-9b3d-b2d9f72ccf05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|RunningTotal|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|           8|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|          17|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|           7|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|          15|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|           5|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|           6|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "window_emp = Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df.withColumn(\"RunningTotal\", sum(\"WorkHours\").over(window_emp)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4b78a9-c056-4643-8a28-d28a7ab562d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Joining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b4b43ef-b2e2-42cf-a469-aa4d1760d646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Join with timesheet data and list all employees with their DeptHead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910c9c39-d6ba-4242-a318-4cadbb0f1d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+--------+\n|EmployeeID| Name|Department|DeptHead|\n+----------+-----+----------+--------+\n|      E101|Anita|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n|      E103| John|   Finance|   Kamal|\n|      E101|Anita|        IT|   Anand|\n|      E104|Meena|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n+----------+-----+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "dept_df = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/department_location.csv\")\n",
    "df_joined = df.join(dept_df, on=\"Department\", how=\"left\")\n",
    "df_joined.select(\"EmployeeID\", \"Name\", \"Department\", \"DeptHead\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3f728f-67f9-4399-b04e-c818203a44e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pivot & Unpivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b4a7e03-fd95-4128-8603-c86b761b26d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. Pivot table: total hours per employee per project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee81cab-149d-4f51-a3ad-e70160b48d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n|EmployeeID|Alpha|Beta|Gamma|\n+----------+-----+----+-----+\n|      E103|    5|NULL| NULL|\n|      E104| NULL|NULL|    6|\n|      E101|   17|NULL| NULL|\n|      E102| NULL|  15| NULL|\n+----------+-----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"EmployeeID\").pivot(\"Project\").agg(sum(\"WorkHours\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0090a35f-76bc-446f-94d4-b71cd4e860f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. Unpivot example: Convert mode-specific hours into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a005bf23-609c-496d-b779-64eb6f3be90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+\n|EmployeeID|  Mode|ModeHours|\n+----------+------+---------+\n|      E104|Onsite|        6|\n|      E102|Remote|        8|\n|      E101|Remote|       17|\n|      E102|Onsite|        7|\n|      E103|Remote|        5|\n+----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_mode = df.select(\"EmployeeID\", \"WorkHours\", \"Mode\")\n",
    "df_mode.groupBy(\"EmployeeID\", \"Mode\").agg(sum(\"WorkHours\").alias(\"ModeHours\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "357cab51-8685-42b8-b2fd-0aa6557e21bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "UDF & Conditional Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29a39301-88ba-4f76-b755-c2d0b655a598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Create a UDF to classify work hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c290432a-b5bb-4754-866e-5337a63256e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8: return \"Full\"\n",
    "    elif hours >= 4: return \"Partial\"\n",
    "    else: return \"Light\"\n",
    "\n",
    "workload_udf = udf(workload_tag, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2a39e5-a7f6-421a-b770-a21257d379ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Add a column WorkloadCategory using this UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c16d024-20c0-4933-a82b-d23b430cfe12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"WorkloadCategory\", workload_udf(col(\"WorkHours\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "734d96a7-9d67-4e36-8365-64bf41e10ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nulls and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eccbbfe-378f-4ea9-8af8-42524e1a36e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15. Introduce some nulls in Mode column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aadf278-b430-4d0e-8115-bcb3ae119b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|  NULL|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null = df.withColumn(\"Mode\", when(col(\"EmployeeID\") == \"E104\", None).otherwise(col(\"Mode\")))\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92b12272-a492-4628-b847-4f3d32fbea11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16. Fill nulls with \"Not Provided\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b7dfa1-90ea-4205-ab1f-8f444abe95b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|      Onsite|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Not Provided|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_filled = df_null.fillna({\"Mode\": \"Not Provided\"})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a3a5a8-5546-4364-a3e7-eec45e3d418d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17. Drop rows where WorkHours < 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e26c4f3-b49c-4d30-a250-c42fd0e160b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|      Onsite|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Not Provided|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_filled.filter(col(\"WorkHours\") >= 4)\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3699f58-d089-4d11-bcb4-33bed831e948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Advanced Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d24bea-7a91-4cb1-bff1-dc439c391250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18. Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries are\n",
    "Remote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a45fd9-94ca-4aee-88dc-2e4290b9c3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+-----------+-------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|RemoteRatio|   WorkerType|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+-----------+-------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|        1.0|Remote Worker|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|        0.5|        Mixed|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|        1.0|Remote Worker|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|        1.0|Remote Worker|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|        0.0|        Mixed|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|        0.5|        Mixed|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "remote_ratio = df.groupBy(\"EmployeeID\") \\\n",
    "    .agg((sum(when(col(\"Mode\") == \"Remote\", 1).otherwise(0)) / count(\"*\")).alias(\"RemoteRatio\"))\n",
    "\n",
    "df_flagged = df.join(remote_ratio, \"EmployeeID\").withColumn(\n",
    "    \"WorkerType\", when(col(\"RemoteRatio\") > 0.8, \"Remote Worker\").otherwise(\"Mixed\")\n",
    ")\n",
    "df_flagged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f467ba-d349-4f87-a568-9d16eccd599c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19. Add a new column ExtraHours where hours > 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a09b9b-c153-4699-8279-86268fc25fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|ExtraHours|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|         0|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|         0|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|         0|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|         1|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|         0|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|         0|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_extra = df.withColumn(\"ExtraHours\", when(col(\"WorkHours\") > 8, col(\"WorkHours\") - 8).otherwise(0))\n",
    "df_extra.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae7d5a9-c918-45f7-8b7b-b61bf6cf4bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Union + Duplicate Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6665caff-2e66-42e0-ba84-8aaa79a1d322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20. Append a dummy timesheet for new interns using unionByName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e1d446-c63e-412b-b1e3-c67e1cfeb421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "intern_data = [(\"E999\", \"Intern\", \"IT\", \"Delta\", 6, \"2024-05-05\", \"Remote\", \"Remote\", \"Sunday\")]\n",
    "\n",
    "columns = df.columns  \n",
    "\n",
    "df_intern = spark.createDataFrame(intern_data, columns)\n",
    "\n",
    "df_combined = df.unionByName(df_intern)\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44863a8c-55c7-4d1c-a0b8-686d5e242aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21. Remove duplicate rows based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12034a02-b9d1-4029-b19e-2710a213b819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dedup = df_combined.dropDuplicates()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}